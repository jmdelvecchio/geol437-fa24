{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to vector and raster data manipulation with `geopandas` and `rasterio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is but a taste of what is possible with geospatial Python. I recommend reading through the \"Getting Started\" chapter in this [online textbook](https://pythongis.org/part2/chapter-05/index.html) before proceeding with the following examples (I will proceed assuming you read that and don't need to learn the basics on geospatial data and/or Python). I also strongly recommend reading through the rest of that section too, especially Vector Processing. The Raster Processing section is also useful, though it will use `xarray`, which we won't explore here (but ask me about it if you're curious). Instead we'll use `rasterio`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will borrow heavily from the NCEAS/Arctic Data Center [workshop material](https://learning.nceas.ucsb.edu/2022-09-arctic/sections/10-geopandas.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, shape\n",
    "import numpy as np\n",
    "import numpy.ma as ma # numpy mask module\n",
    "import urllib\n",
    "from ftplib import FTP\n",
    "\n",
    "import fiona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on Colab, you need to use pip to install\n",
    "\n",
    "# pip install rasterio\n",
    "import rasterio\n",
    "from rasterio import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Table to `geopandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this activity we will look at a very real example of how one of my previous students Sade and I bring together all sorts of exploratory data analysis and tons of different kinds of Python packages. Sade took this [fairly messy data](https://www2.gwu.edu/~calm/data/north.htm) and turned it into something great!\n",
    "\n",
    "This `CALM_export.csv` is actually an export from the shapefile listed on the website above - it was the least messy format of the data. But I just exported it as a .csv so we had an example of turning columns with location data into `GeoDataframes` :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('data/CALM_export.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Latitude.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing Sade did was rename the columns that should be years for the ALT measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_names = np.arange(1990, 2016, 1)\n",
    "\n",
    "old_columns = data.columns[6:32]\n",
    "\n",
    "mapping = {old_columns[i]: year_names[i] for i in range(len(old_columns))}\n",
    "\n",
    "data = data.rename(columns=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when originally imported and/or exported the \"no measurements\" turned into zeros, which is bad, and some of the data had symbols associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no data, fill it with np.nan\n",
    "data = data.fillna(np.nan)\n",
    "# Clean up weird entries\n",
    "data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "data.replace(\">263\", np.nan, inplace= True)\n",
    "data.replace(\">260\", np.nan, inplace= True)\n",
    "data.replace(\">235\", np.nan, inplace= True)\n",
    "# And finally replace float 0.0 values with nans\n",
    "data.replace(0.0, np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6:32].dtypes\n",
    "\n",
    "# iloc is index location\n",
    "# The : in the first half of the bracketed list means \"all rows\"\n",
    "# and then \"column numbers 6 through 32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.columns[6:32]] = data.iloc[:, 6:32].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make it a map! Any dataframe with latitude and longitude data can be convered into a dataframe if we specify the geometry as the appropriate columns and the appropriate crs (lat/long data will usually be WGS84, EPSG:4326). We can use the `gpd.points_from_xy()` method to create a geodataframe from a regular old dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was getting a warning if I had more than one point in the same exact place\n",
    "# For demonstration I'm dropping this but you don't have to do this\n",
    "\n",
    "data = data.drop_duplicates(subset=['Latitude','Longitude'])\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    data, # what dataframe are we using?\n",
    "    geometry=gpd.points_from_xy(\n",
    "        data['Longitude'], data['Latitude']), # here we specify which columns have the long and lat\n",
    "          crs='EPSG:4326' # finally end specifying the WGS84 spatial reference\n",
    "          ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas has a simple map of the Earth built in\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5),dpi=300)\n",
    "im = world.plot(\n",
    "    color='white', edgecolor='black', ax=ax)\n",
    "\n",
    "result = gdf.plot(\n",
    "        ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_ylim(40,90)\n",
    "\n",
    "ax.set_ylabel(\"ylabel\")\n",
    "ax.set_xlabel(\"xlabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's export this as a shapefile for use later down the road!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A goofy little thing where the numbers have to be strings to export lol\n",
    "gdf.columns = gdf.columns.astype(str)\n",
    "\n",
    "gdf.dropna(inplace=True, subset='Site_Name')\n",
    "\n",
    "gdf.to_file(\"CALM_points.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Raster data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are going to pull data directly off of the USGS server. The URL is for the data entry, which is the first argument in  `urlretrieve`, and the second argument is the specific file to be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://prd-tnm.s3.amazonaws.com/StagedProducts/Elevation/1/TIFF/historical/n39w079/USGS_1_n39w079_20211229.tif'\n",
    "\n",
    "filename = 'data/USGS_1_n39w079_20211229.tif'\n",
    "\n",
    "msg = urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `.tif` we just downloaded. The `with rasterio.open() as topo:` syntax saves us some memory by \"opening\" and then \"closing\" the raster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "with rasterio.open(filename, masked=True) as topo:\n",
    "    # read in raster (1st band)\n",
    "    topo_arr = topo.read(1)\n",
    "    topo_arr = ma.masked_where(topo_arr == topo.nodata, topo_arr)\n",
    "    topo_meta = topo.profile\n",
    "\n",
    "im0 = ax.imshow(topo_arr)\n",
    "cb = fig.colorbar(im0, ax=ax, label=\"DEM\",  orientation='horizontal',\n",
    "                #    fraction=0.04, pad=0.2\n",
    "                   )\n",
    "print(topo_meta)\n",
    "ax.set_ylabel(\"Pixels\")\n",
    "ax.set_xlabel(\"Pixels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here?\n",
    "- we instantiated a `fig` and `ax` object\n",
    "- we used `numpy.ma` to mask out the nodata values\n",
    "- we printed out the metadata for the raster\n",
    "- we used `imshow()` on the `ax` object to display the values of the array \n",
    "- the axes have an origin at the bottom left of (0,3600) because we did not provide `imshow` an `extent`, which we will see later, that can put the x and y axes in real space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rasterio` has some built-in plotting functions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.plot import show\n",
    "with rasterio.open(filename, masked=True) as topo:\n",
    "    show(topo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Complex vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio = gpd.read_file('data/physio.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so what do we have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, geopandas GeoDataFrames are just dataframes with some spatial info! We can look at the columns to see attributes for the shapefiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`geopandas` has a `.plot()` method which allows you to directly plot the geometries associated with your new shapefile. `plot()` takes all sorts of arguments - check out the docs. The most useful one is to specify a column by which to color the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio.plot(column=\"DIVISION\", legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn! Please produce a map using a different column in the geodataframe. I am going to give you example code for creating an object-oriented geopandas plot, which specifies an axis object to place the plot on instead of just calling the `.plot()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = #what might go here?\n",
    "physio_map = physio.plot(\n",
    "                        column=#\"choose something else\",\n",
    "                        legend=True,\n",
    "                        ax=#what's here?\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with (re)projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the coordinate reference system of a GeoDataFrame with a call to `.crs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio.crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's very simple to reproject GeoDataFrames! Let's transform this to a [coordinate reference system](https://geopandas.org/en/stable/docs/user_guide/projections.html) that is usually for polar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio.to_crs(\"EPSG:3338\").plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that geopandas is pretty good at reprojecting, but man are conical polar projections tricky math!\n",
    "\n",
    "Let's clip the permafrost shapefile to the extent of our elevation raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(filename, masked=True) as topo:\n",
    "    # Use Shapely to make a box geodataframe object from the bounds of the raster\n",
    "    bbox = box(topo.bounds[0], topo.bounds[1], topo.bounds[2], topo.bounds[3])\n",
    "    # Make sure your bounding box has the crs of the raster\n",
    "    coord_box_df = gpd.GeoDataFrame(crs = topo.crs, geometry = [bbox])\n",
    "\n",
    "print(f'Bounding box crs is {coord_box_df.crs.to_epsg()}')\n",
    "print(f'Physio  crs is {physio.crs}')\n",
    "\n",
    "# If they match you can immediately clip one with the other!\n",
    "\n",
    "physio_clip = physio.clip(coord_box_df)\n",
    "\n",
    "physio_clip.plot(column='PROVINCE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: together!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rasterio.plot.show(src)` can plot raster data. But rasters are just grids (or arrays) of numbers. If you tell `plt.imshow()` the extent (boundaries) of the image (the data array), you can transform your raster data however you want, and you can plot it with vector data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "with rasterio.open(filename, masked=True) as topo:\n",
    "    # read in raster (1st band)\n",
    "    topo_arr = topo.read(1)\n",
    "    topo_arr = ma.masked_where(topo_arr == topo.nodata, topo_arr)\n",
    "    \n",
    "    \n",
    "    # Don't you love how \"extent\" coordinates need to be in a different order? :upside-down smiley face:\n",
    "    extent=[topo.bounds[0], topo.bounds[2], topo.bounds[1], topo.bounds[3]]\n",
    "    \n",
    "    \n",
    "    im0 = ax.imshow(topo_arr,\n",
    "    extent=extent,\n",
    "    cmap=\"binary_r\",\n",
    "    zorder=0,\n",
    "    )\n",
    "\n",
    "    physio_shape = physio_clip.plot(column=\"PROVINCE\", legend=True, alpha=0.5, zorder=2, ax=ax)\n",
    "\n",
    "cb = fig.colorbar(im0, ax=ax, label=\"Elevation (m)\",\n",
    "                    # orientation='horizontal', fraction=0.04, pad=0.1\n",
    "                    )\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.set_ylabel(\"??\")\n",
    "ax.set_xlabel(\"??\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Split-apply-combine raster data with vector data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to analyze raster data grouped by vector data properties? The strategy we have here is to [`rasterize`](https://rasterio.readthedocs.io/en/latest/api/rasterio.features.html#rasterio.features.rasterize) our vector data - instead of having polygons with certain properties, we're going to create arrays where there is one value \"under\" the polygon of interest and another value (0, or null) everywhere else"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to [`dissolve`](https://geopandas.org/en/stable/docs/user_guide/aggregation_with_dissolve.html) our polygons based on their `PROVINCE` values so that we are rasterizing all the values we are interested in at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio_clip = physio_clip.dissolve(by='PROVINCE').reset_index()\n",
    "# resetting the index helps us iterate through the geometries in the next step, though note each NUM_CODE is unique to the COMBO\n",
    "physio_clip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to instantiate two [dictionaries](https://docs.python.org/3/tutorial/datastructures.html) to store the resulting data arrays.\n",
    "\n",
    "Why dictionaries? You typically don't want to store lists or arrays in pandas dataframes. Your goal is to store big raw datasets in dictionaries, reduce them, and then stick them back into your dataframe when all is said and done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two empty dictionaries to be filled with data\n",
    "means_dict = {}\n",
    "raw_values_dict = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we shall \n",
    "- (1) rasterize our polygons, \n",
    " - (2) make a mask over the raster where the polygon value is present, and both \n",
    " - (3) store that raw masked array and \n",
    " - (4) calculate the mean of that masked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for geom, idx in zip(physio_clip['geometry'], physio_clip.index):\n",
    "    # I spent WAY too much time messing around with this part, you apparently can't just\n",
    "    # give rasterize a polygon, it has to either be a MutliPolygon or a list of geometries\n",
    "    # This code does the latter, and the Arctic Data Center tutorial's only works because their\n",
    "    # example vector data is accidentally multipolygons, but our data has a polygon\n",
    "    geom = shape(geom)\n",
    "    geoms = [(geom, 1)]\n",
    "    # Now this looks like the ADC example\n",
    "    rasterized = features.rasterize(\n",
    "                                    geoms,\n",
    "                                    out_shape=topo_arr.shape, # Look like the source raster\n",
    "                                    transform=topo_meta['transform'], # Have the geometry of the source raster\n",
    "                                    all_touched=True # all pixels touched by geometries (as opposed to pixel centers)\n",
    "                                    )\n",
    "    # Theoretically instead of individually rasterizing each polygon type\n",
    "    # You could rasterize the whole thing and instead of 0s and 1s you can\n",
    "    # make the raster value the index and then do basic array-style analyses\n",
    "    # Maybe I can offer treats to someone who writes that for me...\n",
    "    r_index = np.where(rasterized == 1) # Make the mask\n",
    "    raw_values_dict[idx] = topo_arr[r_index].compressed() # store all non-masked data (the compressed thing)\n",
    "    means_dict[idx] = np.nanmean(topo_arr[r_index].compressed()) # calculate the mean of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dictionary whose keys are the index and the values are the means of the masked arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I also made a dictionary that stored *all* of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_values_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you want to do further analyses on the data that you don't want to reduce in that loop. For example, maybe you want to set a threshold number of pixels before you allow for a mean to be counted? This is relevant in this example where the polygon for \"ocean\" has some data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len is length of the array, or number of unmasked pixels \n",
    "{key: len(value) for key, value in raw_values_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just create a data frame from that dictionary, and join it to the vector data using `pandas` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame from the result\n",
    "means_df = pd.DataFrame.from_dict(means_dict,\n",
    "                                     orient='index',\n",
    "                                     columns=['mean_elev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pd.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) is a common and powerful way to join two dataframes that share common columns or indices (the latter is true in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physio_data = physio_clip.merge(means_df,\n",
    "                                    left_index=True,\n",
    "                                    right_index=True,\n",
    "                                    how='inner')\n",
    "\n",
    "physio_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a map! I took some inspiration for treating missing data from the [GeoPandas docs](https://geopandas.org/en/stable/docs/user_guide/mapping.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "res = physio_data.plot(column=\"mean_elev\",\n",
    "                        legend=True,\n",
    "                        legend_kwds={'label': \"Elevation (mean)\",\n",
    "                        'orientation': \"vertical\"},\n",
    "                        missing_kwds={\n",
    "                        \"color\": \"lightgrey\",\n",
    "                        \"edgecolor\": \"red\",\n",
    "                        \"hatch\": \"///\",\n",
    "                        \"label\": \"No values\",\n",
    "                        },\n",
    "                        ax=ax\n",
    "                        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to just install and use [`rasterstats`](https://github.com/perrygeo/python-rasterstats/tree/d188eaf1f1c20c3ef33aad407f55f9fce51a1220), whose source code I have stolen to write the above snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note: point data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can use `rasterio` and `geopandas` to [sample](https://rasterio.readthedocs.io/en/latest/api/rasterio.sample.html) point data. A good example can be found in the [GeoPandas docs](https://geopandas.org/en/stable/gallery/geopandas_rasterio_sample.html). If you want to sample an area around a point, consider employing a [buffer](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.buffer.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neukom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4496227e9e35a1cad8f46d1878e766ce3696b74827c1cccb91d7e0ed1733d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
